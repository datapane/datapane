{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90c6033-629a-4a95-988b-74ebeb15749d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Accessing data in Python\n",
    "\n",
    "## Introduction\n",
    "\n",
    "To build Datapane reports, you'll usually start by importing data into your Python environment. This is an overview of the main data sources and how to access them:&#x20;\n",
    "\n",
    "1. Local files\n",
    "2. Databases\n",
    "3. APIs\n",
    "4. Data access libraries\n",
    "\n",
    "The only major requirement is installing the `pandas` library:&#x20;\n",
    "\n",
    "\n",
    "=== \"CLI\"\n",
    "\n",
    "    ```shell\n",
    "    $ pip install pandas\n",
    "    OR\n",
    "    $ conda install pandas\n",
    "    ```\n",
    "\n",
    "=== \"Jupyter\"\n",
    "\n",
    "    ```shell\n",
    "    !pip install pandas\n",
    "    OR\n",
    "    !conda install pandas\n",
    "    ```\n",
    "    \n",
    "## Local files\n",
    "\n",
    "Often the data you need is stored in a local file on your computer. Depending on where you're running your Python environment, you can either specify the filename as a [relative or absolute path](https://www.earthdatascience.org/courses/intro-to-earth-data-science/python-code-fundamentals/work-with-files-directories-paths-in-python/):&#x20;\n",
    "\n",
    "```python\n",
    "# Absolute path\n",
    "file1 = \"~/Users/johnreid/Documents/my_project/data/example.csv\"\n",
    "# Relative path, assuming current working directory is my_project\n",
    "file2 = \"./data/example.csv\"\n",
    "```\n",
    "\n",
    "### **CSV files**\n",
    "\n",
    "CSVs are a popular choice for storing tabular data, here we'll look at a population dataset from[ Our World in Data](https://ourworldindata.org/grapher/population-by-country) using `pd.read_csv`:&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c9082e5-b971-4e52-87a7-50d63f7a047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = \"population-by-country.csv\"\n",
    "df_from_csv = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d875ce9-a451-4a9d-b494-aa319cca8bf9",
   "metadata": {},
   "source": [
    "After importing the data, it's helpful to run `df.info()` to understand how your data is structured e.g. how many rows, columns and non-null values you have. Running that code gives us the following output:&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89eda6db-c7dd-4da0-8d2c-7029626b65ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3221 entries, 0 to 3220\n",
      "Data columns (total 4 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   Entity                               3221 non-null   object \n",
      " 1   Code                                 3130 non-null   object \n",
      " 2   Year                                 3221 non-null   int64  \n",
      " 3   Total population (Fink-Jensen 2015)  3221 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 100.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_from_csv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f127f6e0-e85b-4e0c-a21a-5fd34d916521",
   "metadata": {},
   "source": [
    "!!! info\n",
    "    \n",
    "    If you keep getting  a`FileNotFoundError`, try renaming your filename to replace spaces with underscores e.g. \"Financial Sample.xlsx\" becomes \"Financial\\_Sample.xlsx\".&#x20;\n",
    "\n",
    "### **Excel files**\n",
    "\n",
    "You need to be a bit more cautious with Excel files, because they may contain more than one sheet of data and complex visual formatting e.g. extra header rows. Otherwise the syntax is pretty similar - here's a [financial data](https://go.microsoft.com/fwlink/?LinkID=521962) example:&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a125c6af-6a8a-4586-84af-9373de2310d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 700 entries, 0 to 699\n",
      "Data columns (total 16 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   Segment              700 non-null    object        \n",
      " 1   Country              700 non-null    object        \n",
      " 2   Product              700 non-null    object        \n",
      " 3   Discount Band        700 non-null    object        \n",
      " 4   Units Sold           700 non-null    float64       \n",
      " 5   Manufacturing Price  700 non-null    int64         \n",
      " 6   Sale Price           700 non-null    int64         \n",
      " 7   Gross Sales          700 non-null    float64       \n",
      " 8   Discounts            700 non-null    float64       \n",
      " 9    Sales               700 non-null    float64       \n",
      " 10  COGS                 700 non-null    float64       \n",
      " 11  Profit               700 non-null    float64       \n",
      " 12  Date                 700 non-null    datetime64[ns]\n",
      " 13  Month Number         700 non-null    int64         \n",
      " 14  Month Name           700 non-null    object        \n",
      " 15  Year                 700 non-null    int64         \n",
      "dtypes: datetime64[ns](1), float64(6), int64(4), object(5)\n",
      "memory usage: 87.6+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "excel_file = \"Financial Sample.xlsx\"\n",
    "df_from_excel = pd.read_excel(excel_file, sheet_name=\"Sheet1\")\n",
    "df_from_excel.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aa493a-0c34-417a-afcf-7c46d80b31b5",
   "metadata": {},
   "source": [
    "### **Text**\n",
    "\n",
    "Text files often need more data processing - start by looking at how the data is stored and how you'd like to represent it in Python. From there, you can write code to transform textual input into a dataframe. Let's use a shopping list example, with each line containing an item and a quantity:&#x20;\n",
    "\n",
    "```text title=\"shopping_list.txt\"\n",
    "Eggs 6\n",
    "Milk 11\n",
    "Apples 6\n",
    "Cheese 250g\n",
    "Flour 2.5kg\n",
    "```\n",
    "\n",
    "To convert that to a dataframe, you can try the following:&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ebbe4eb-16eb-40d5-851c-31aaca665ce6",
   "metadata": {
    "tags": [
     "remove_all_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Report saved to ./accessing-text-data.html"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datapane as dp\n",
    "\n",
    "shopping_list = \"shopping_list.txt\"\n",
    "\n",
    "results = []\n",
    "\n",
    "with open(shopping_list) as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        results.append(line.strip().split(\" \"))\n",
    "        line = f.readline()\n",
    "\n",
    "f.close()\n",
    "\n",
    "df_from_textfile = pd.DataFrame(results, columns=[\"Item\", \"Quantity\"])\n",
    "\n",
    "report = dp.Report(df_from_textfile)\n",
    "report.save(path=\"accessing-text-data.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38706bb6-5326-414f-97be-f927cc813ddd",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/guides/accessing-data-in-python/accessing-text-data.html\"><img src=\"/guides/accessing-data-in-python/accessing-text-data-preview.png\"></a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dpdocsutils import previews\n",
    "previews.embed_local_report('/guides/accessing-data-in-python/accessing-text-data.html', width=\"100%\", height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd311b83-aa29-4083-b59d-04e7f07b736f",
   "metadata": {},
   "source": [
    "We read the lines one-by-one, strip extra whitespaces and split the line into two parts. When we create a dataframe, we also need to assign column names.\n",
    "\n",
    "### **Multiple files / folders**\n",
    "\n",
    "&#x20;Let's combine a couple of things that we've learned to extract data from the [BBC Sport text dataset](http://mlg.ucd.ie/datasets/bbc.html).\n",
    "\n",
    "![](../../img/guides/accessing-data-in-python-1.png)\n",
    "\n",
    "We have 5 subfolders, each with around 100 files. Each file starts with a headline, followed by the body of the article. Our goal will be to combine all these files into a single dataframe with 'Title', 'Subtitle', 'Body' and 'Genre' columns. The `glob` library comes really in handy:&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2567e998-13ae-4ae1-b9a1-30968c5f42cb",
   "metadata": {
    "tags": [
     "remove_all_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Report saved to ./accessing-multiple-file-data.html"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "base_path = \"./bbcsport/\"\n",
    "genres = [\"athletics\", \"cricket\", \"football\", \"rugby\", \"tennis\"]\n",
    "\n",
    "\n",
    "def read_and_split_file(filename):\n",
    "    with open(filename, \"r\", encoding=\"latin-1\") as f:\n",
    "        lines = f.readlines()  # Get lines as a list of strings\n",
    "        lines = list(map(str.strip, lines))  # Remove /n characters\n",
    "        lines = list(filter(None, lines))  # Remove empty strings\n",
    "    return lines\n",
    "\n",
    "\n",
    "def get_df_from_genre(path, genre):\n",
    "    files = glob.glob(path + genre + \"/*.txt\")\n",
    "    titles = []\n",
    "    subtitles = []\n",
    "    bodies = []\n",
    "\n",
    "    for f in files:\n",
    "        lines = read_and_split_file(f)\n",
    "        titles.append(lines[0])  # First line is the title\n",
    "        subtitles.append(lines[1])  # Second line is the subtitle\n",
    "        bodies.append(\" \".join(lines[2:]))  # Combine all the rest\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\"genre\": genre, \"title\": titles, \"subtitle\": subtitles, \"body\": bodies}\n",
    "    )\n",
    "\n",
    "\n",
    "final_df = pd.concat([get_df_from_genre(base_path, g) for g in genres])\n",
    "report = dp.Report(final_df)\n",
    "report.save(path=\"accessing-multiple-file-data.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336103b-5aee-4bfb-a4dd-00634b99c1ce",
   "metadata": {},
   "source": [
    "Note that you can concatenate multiple dataframes together using `pd.concat`.&#x20;\n",
    "\n",
    "Running that code gives us the following output:&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "233ca445-b93e-4cce-96a9-695ebde23cb2",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/guides/accessing-data-in-python/accessing-multiple-file-data.html\"><img src=\"/guides/accessing-data-in-python/accessing-multiple-file-data-preview.png\"></a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dpdocsutils import previews\n",
    "previews.embed_local_report('/guides/accessing-data-in-python/accessing-multiple-file-data.html', width=\"100%\", height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4739c39-9501-46c7-a6a6-e5c0cffd94f6",
   "metadata": {},
   "source": [
    "## Databases\n",
    "\n",
    "Most organizations store their business-critical data in a [relational database](https://en.wikipedia.org/wiki/Relational\\_database) like Postgres or MySQL, and you’ll need to know **S**tructured **Q**uery **L**anguage (SQL) to access or update the data stored there.\n",
    "\n",
    "### SQLite\n",
    "\n",
    "SQLite is an embedded database that is stored as a single file, so it's a great place to start testing out queries. Here we'll show an example of connecting to a SQLite file of the [Chinook](https://github.com/lerocha/chinook-database) database:&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e142bf6d-2e4c-463f-8cb0-8be11eda9b57",
   "metadata": {
    "tags": [
     "remove_all_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Report saved to ./accessing-db-data.html"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "\n",
    "conn = sql.connect(\"Chinook_Sqlite.sqlite\")\n",
    "\n",
    "# First pattern - turn query directly into dataframe:\n",
    "df1 = pd.read_sql_query(\"SELECT * FROM invoice\", conn)\n",
    "\n",
    "# Second pattern - get row-level data, but no column names\n",
    "cur = conn.cursor()\n",
    "results = cur.execute(\"SELECT * FROM invoice LIMIT 5\").fetchall()\n",
    "df2 = pd.DataFrame(results)\n",
    "report = dp.Report(df2)\n",
    "report.save(path=\"accessing-db-data.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4042576-57ff-4fac-b2b2-6e0e56f84965",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/guides/accessing-data-in-python/accessing-db-data.html\"><img src=\"/guides/accessing-data-in-python/accessing-db-data-preview.png\"></a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dpdocsutils import previews\n",
    "previews.embed_local_report('/guides/accessing-data-in-python/accessing-db-data.html', width=\"100%\", height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a002a27a-5fd3-417b-a089-6a8b37958ab4",
   "metadata": {},
   "source": [
    "### Remote databases\n",
    "\n",
    "Connecting to a remote database like Postgres, Redshift, or SQLServer uses mostly the same syntax but requires access credentials. For security reasons, it's best to store these credentials in a config file and load them into your Python script. You can create a separate `.py` file like this:&#x20;\n",
    "\n",
    "```python title=\"config.py\"\n",
    "host = \"localhost\"\n",
    "database= \"suppliers\"\n",
    "user = \"postgres\"\n",
    "password = \"SecurePas$1\"\n",
    "```\n",
    "and then import it into your Python script as follows (you'll also need the `psychopg2` library):&#x20;\n",
    "\n",
    "```python\n",
    "import psycopg2\n",
    "import config\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=config.host,\n",
    "    database=config.database,\n",
    "    user=config.user,\n",
    "    password=config.password,\n",
    ")\n",
    "    \n",
    "## Run queries to your heart's delight!\n",
    "```\n",
    "\n",
    "!!! info\n",
    "    \n",
    "    Make sure to keep your `config.py` file safe and don't upload it elsewhere - you can add it to your `.gitignore` to make sure it doesn't get included in git commits.&#x20;\n",
    "\n",
    "### **SQLAlchemy**\n",
    "\n",
    "If you want a more ‘pythonic’ way of querying a database, try the [SQLAlchemy](https://www.sqlalchemy.org/) library, which is an Object-Relational-Mapper. It’s typically used for applications so that developers don’t have to write pure SQL to update their database, but you can use it for querying data too!\n",
    "\n",
    "Here’s an example using the same Chinook music store database:&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6030762-5ee2-43b7-a856-9f7ccddabc5f",
   "metadata": {
    "tags": [
     "remove_all_output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahin/Documents/datapane-docs-beta/.venv/lib/python3.10/site-packages/pandas/io/sql.py:1402: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage.\n",
      "  return self.connectable.execution_options().execute(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Report saved to ./accessing-sqlalchemy-data.html"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sqlalchemy as db\n",
    "import pandas as pd\n",
    "\n",
    "engine = db.create_engine(\"sqlite:///Chinook_Sqlite.sqlite\")\n",
    "connection = engine.connect()\n",
    "metadata = db.MetaData()\n",
    "invoice = db.Table(\"invoice\", metadata, autoload=True, autoload_with=engine)\n",
    "\n",
    "# Get the first 10 invoices from the USA\n",
    "query = db.select([invoice]).limit(5)\n",
    "\n",
    "df = pd.read_sql(query, engine)\n",
    "report = dp.Report(df)\n",
    "report.save(path=\"accessing-sqlalchemy-data.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb14d7-66c9-4bbd-860b-9a8e6c8441da",
   "metadata": {},
   "source": [
    "In this code we connect to the database, then set up some tables & metadata in SQLAlchemy. Once that’s defined, we can write a query in a more ‘pythonic’ way and read the results directly to a Pandas dataframe. Running that code gives the following output:&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea40a91e-1f54-46ab-b140-25fabe7815c4",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/guides/accessing-data-in-python/accessing-sqlalchemy-data.html\"><img src=\"/guides/accessing-data-in-python/accessing-sqlalchemy-data-preview.png\"></a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dpdocsutils import previews\n",
    "previews.embed_local_report('/guides/accessing-data-in-python/accessing-sqlalchemy-data.html', width=\"100%\", height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51afe503-a274-4aae-9d4a-908bd7028db5",
   "metadata": {},
   "source": [
    "## APIs&#x20;\n",
    "\n",
    "Sometimes you'll need to access data from a particular platform your company uses, like Hubspot, Twitter or Trello. These platforms often have a public API that you can pull data from, directly inside your Python environment.&#x20;\n",
    "\n",
    "The basic idea is you send a request (which may include query parameters and access credentials) to an endpoint. That endpoint will return a response code plus the data you asked for (hopefully). The most [common response codes](https://www.restapitutorial.com/httpstatuscodes.html) are:&#x20;\n",
    "\n",
    "* `200`: Everything went okay, and the result has been returned.\n",
    "* `301`: The server is redirecting you to a different endpoint. This can happen when a company switches domain names, or an endpoint name is changed.\n",
    "* `400`: The server thinks you made a bad request. This can happen when you don’t send along the right data, among other things.\n",
    "* `403`: The resource you’re trying to access is forbidden: you don’t have the right permissions to see it.\n",
    "* `404`: The resource you tried to access wasn’t found on the server.\n",
    "* `503`: The server is not ready to handle the request.\n",
    "\n",
    "You'll need to look at the API documentation to understand what data fields are available. The data will usually be returned in JSON format, which allows for deeply-nested data. Let's do a minimal example using the [OpenNotify](http://open-notify.org/Open-Notify-API/People-In-Space/) API, which tracks all the people currently in space:&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d3a25b-ac30-43da-99cb-1ea65ae4d61b",
   "metadata": {
    "tags": [
     "remove_all_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Report saved to ./accessing-api-data.html"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://api.open-notify.org/astros.json\")\n",
    "res = pd.DataFrame(response.json()[\"people\"])\n",
    "report = dp.Report(res)\n",
    "report.save(path=\"accessing-api-data.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff3630-bec8-48b0-9c1b-0d1663d1b54c",
   "metadata": {},
   "source": [
    "Running that code gives us the following output:&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22a305f7-d49b-460d-826e-f51db978b24c",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/guides/accessing-data-in-python/accessing-api-data.html\"><img src=\"/guides/accessing-data-in-python/accessing-api-data-preview.png\"></a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dpdocsutils import previews\n",
    "previews.embed_local_report('/guides/accessing-data-in-python/accessing-api-data.html', width=\"100%\", height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b9773b-344e-4b80-82d6-ac002e8fafd0",
   "metadata": {},
   "source": [
    "From here, try including query parameters or access credentials for your favourite API!&#x20;\n",
    "\n",
    "!!! info\n",
    "    \n",
    "    If you don't want to deal with JSON you can try searching for a Python library for that API - these are usually open-source and maintained by the company or third parties.&#x20;\n",
    "\n",
    "## Data Access Libraries\n",
    "\n",
    "### **Pandas\\_datareader**\n",
    "\n",
    "[Pandas\\_datareader](https://pandas-datareader.readthedocs.io/en/latest/index.html) is a great way to pull data from the internet into your Python environment. It is particularly suited to financial data, but also has some World Bank datasources. To get Zoom's daily share price over the past few years, try the following:&#x20;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62143af-2460-48ea-a369-8b24decaa28d",
   "metadata": {},
   "source": [
    "```ipython\n",
    "!pip install pandas_datareader\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8de4d563-94ac-4a0d-aba7-32b01441e5f1",
   "metadata": {
    "tags": [
     "remove_all_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Report saved to ./accessing-pandasdatareader-data.html"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas_datareader import data\n",
    "import datetime as dt\n",
    "\n",
    "# Only get the adjusted close.\n",
    "zm = data.DataReader(\n",
    "    \"ZM\", start=\"2019-1-1\", end=dt.datetime.today(), data_source=\"yahoo\"\n",
    ").reset_index()\n",
    "\n",
    "report = dp.Report(zm)\n",
    "report.save(path=\"accessing-pandasdatareader-data.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e5997-a34d-4bef-a4db-2d11c89d3607",
   "metadata": {},
   "source": [
    "Running that code gives us the following output:&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9d8694c-2728-418d-b111-58c610f8b92b",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/guides/accessing-data-in-python/accessing-pandasdatareader-data.html\"><img src=\"/guides/accessing-data-in-python/accessing-pandasdatareader-data-preview.png\"></a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dpdocsutils import previews\n",
    "previews.embed_local_report('/guides/accessing-data-in-python/accessing-pandasdatareader-data.html', width=\"100%\", height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7df210-c427-42e5-ae5a-88d4c474be06",
   "metadata": {},
   "source": [
    "### **DataCommons**\n",
    "\n",
    "[Datacommons](https://datacommons.org/) is a project by Google providing access to standardized and cleaned public datasets. The underlying data is represented in a graph format, making it really easy to [query and join data](https://towardsdatascience.com/exploring-datacommons-the-api-powering-google-search-afc366ec242b) from many different datasources e.g. the US Census, World Bank, Wikipedia, Centre for Disease Control and more. Here's a basic example:&#x20;\n",
    "\n",
    "```python\n",
    "!pip install datacommons datacommons_pandas --upgrade --quiet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58a213eb-7266-4ec0-ba49-fabec89ce297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Report saved to ./accessing-datacommons-data.html"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "How is your experience of Datapane? Please take two minutes to answer our anonymous product survey <a href='https://bit.ly/3lWjRlr' target='_blank'>here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datacommons, datacommons_pandas as dc\n",
    "import pandas as pd\n",
    "\n",
    "city_dcids = dc.get_property_values([\"CDC500_City\"], \"member\", limit=500)[\"CDC500_City\"]\n",
    "\n",
    "cdc500_df = dc.build_multivariate_dataframe(\n",
    "    city_dcids,\n",
    "    [\n",
    "        \"Percent_Person_Obesity\",  # Prevalence of obesity from CDC\n",
    "        \"Median_Income_Person\",\n",
    "        \"Median_Age_Person\",\n",
    "        \"UnemploymentRate_Person\",  # Unemployment rate from BLS\n",
    "        \"Count_Person_BelowPovertyLevelInThePast12Months\",  # Persons living below the poverty line from Census\n",
    "        \"Count_Person\",  # Total population from Census\n",
    "    ],\n",
    ")\n",
    "\n",
    "report = dp.Report(cdc500_df)\n",
    "report.save(path=\"accessing-datacommons-data.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a6a04-da1c-46c6-be4e-2eb1af25175d",
   "metadata": {},
   "source": [
    "Running that code gives us the following:&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "650b981d-eb58-432c-931d-f9711eda5f0e",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/guides/accessing-data-in-python/accessing-datacommons-data.html\"><img src=\"/guides/accessing-data-in-python/accessing-datacommons-data-preview.png\"></a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dpdocsutils import previews\n",
    "previews.embed_local_report('/guides/accessing-data-in-python/accessing-datacommons-data.html', width=\"100%\", height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0829074-0b1b-4c64-9c29-846fbb968ed4",
   "metadata": {},
   "source": [
    "### **PyTrends (Google Trends)**\n",
    "\n",
    "[PyTrends](https://github.com/GeneralMills/pytrends#interest-over-time) is an unofficial but useful library for querying [Google Trends](https://trends.google.com/trends/explore?date=today%205-y\\&q=oat%20milk,soy%20milk,almond%20milk) data - here's a simple example:&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11027291-20e4-4a5a-a410-1c7293baf5ca",
   "metadata": {
    "tags": [
     "remove_all_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Report saved to ./accessing-pytrends-data.html"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "pytrends = TrendReq()\n",
    "keywords = [\"oat milk\", \"soy milk\", \"almond milk\"]\n",
    "pytrends.build_payload(\n",
    "    keywords, cat=0, geo=\"\", gprop=\"\"\n",
    ")  # Get data from the last 5 years\n",
    "top_queries = pytrends.interest_over_time()[keywords]\n",
    "\n",
    "top_queries.head()\n",
    "report = dp.Report(cdc500_df)\n",
    "report.save(path=\"accessing-pytrends-data.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea15fab-9586-4476-b4fc-942c6c605a0d",
   "metadata": {},
   "source": [
    "Running that code gives us the following output:&#x20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b6e6d65-3daf-42a9-9fe6-34dfb1978cf6",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/guides/accessing-data-in-python/accessing-pytrends-data.html\"><img src=\"/guides/accessing-data-in-python/accessing-pytrends-data-preview.png\"></a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dpdocsutils import previews\n",
    "previews.embed_local_report('/guides/accessing-data-in-python/accessing-pytrends-data.html', width=\"100%\", height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abebac92-bc62-47c0-b0cc-50a91ae1e33c",
   "metadata": {},
   "source": [
    "### **Kaggle**\n",
    "\n",
    "Kaggle is a data science community that hosts a lot of datasets and competitions for learning Python. You can download some of these datasets to play around with through their command-line interface (note: you'll need to sign up for a Kaggle account). For example, say we want to download some [Zillow economics data](https://www.kaggle.com/zillow/zecon):&#x20;\n",
    "\n",
    "\n",
    "=== \"CLI\"\n",
    "    ```shell\n",
    "    $ pip install kaggle\n",
    "    $ export KAGGLE_USERNAME=datadinosaur\n",
    "    $ export KAGGLE_KEY=xxxxxxxxxxxxxx\n",
    "    $ kaggle datasets download zillow/zecon\n",
    "    $ unzip zecon.zip\n",
    "    ```\n",
    "\n",
    "This will download a zipped file of the datasets, and then uncompress them. From there, you can open them as local files with Pandas:&#x20;\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "csv_file = \"Zip_time_series.csv\"\n",
    "df_from_csv = pd.read_csv(csv_file)\n",
    "df_from_csv.info()\n",
    "```\n",
    "\n",
    "To learn more, check out the [Kaggle API documentation](https://github.com/Kaggle/kaggle-api). &#x20;"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
